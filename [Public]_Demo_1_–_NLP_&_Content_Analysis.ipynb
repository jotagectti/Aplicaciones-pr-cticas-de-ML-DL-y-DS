{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jotagectti/Aplicaciones-pr-cticas-de-ML-DL-y-DS/blob/main/%5BPublic%5D_Demo_1_%E2%80%93_NLP_%26_Content_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6rjRQDH93cP"
      },
      "source": [
        "# Workshop: NLP & Content Analysis\n",
        "\n",
        "This workshop session will provide a hands-on experience for you to learn modern \"out-of-the-box\" natural language processing techniques that will allow you to design systematic methods to analize text.\n",
        "\n",
        "**During this lab you will**:\n",
        "\n",
        "- Get hands-on experience through the following modules:\n",
        "  - Sentiment lexicon\n",
        "  - Valence recognition\n",
        "  - Perspective API\n",
        "  - Word embeddings\n",
        "  - Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXr0TbU_oGF"
      },
      "source": [
        "# 0. Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please make sure to change the runtime of this Colab notebook to use GPUs.** Select 'Runtime' --> 'Change runtime type' --> select 'GPU' for the Hardware accelerator. GPUs are optimized for graphics-related computations, which involve many matrix multiplications. It turns out that matrix multiplications are similarly pervasive in modern machine learning (i.e. deep neural networks), and GPUs can greatly increase the speed of using these models."
      ],
      "metadata": {
        "id": "AszsjdU0dpuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uk2nO0C5CzTv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gensim\n",
        "!pip install sentence-transformers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 0)\n",
        "pd.set_option('display.max_columns', 999)\n",
        "\n",
        "import itertools\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gensim\n",
        "import scipy\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzVdhct0tzSN"
      },
      "source": [
        "#### Download data and models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJKeFQjRIEWm"
      },
      "source": [
        "The following code downloads the data and our toolkit for exploring this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JRi-5A6OoFaq",
        "outputId": "ad41707c-68d2-4c58-f2de-a668c0835a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ogRFqUjIUqBxIUkGX6z8DobOWonOJp_u\n",
            "To: /content/reddit_workshop_sample.csv\n",
            "\r  0% 0.00/788k [00:00<?, ?B/s]\r100% 788k/788k [00:00<00:00, 162MB/s]\n",
            "--2023-01-10 15:19:59--  https://public-thought.media.mit.edu/static/ccc_toolkit_v_21_0.py\n",
            "Resolving public-thought.media.mit.edu (public-thought.media.mit.edu)... 18.27.78.114\n",
            "Connecting to public-thought.media.mit.edu (public-thought.media.mit.edu)|18.27.78.114|:443... connected.\n",
            "WARNING: cannot verify public-thought.media.mit.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
            "  Issued certificate has expired.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13578 (13K) [application/octet-stream]\n",
            "Saving to: ‘ccc_toolkit_v_21_0.py’\n",
            "\n",
            "ccc_toolkit_v_21_0. 100%[===================>]  13.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-10 15:20:00 (280 MB/s) - ‘ccc_toolkit_v_21_0.py’ saved [13578/13578]\n",
            "\n",
            "--2023-01-10 15:20:00--  https://saifmohammad.com/WebDocs/Lexicons/NRC-Emotion-Lexicon.zip\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25878449 (25M) [application/zip]\n",
            "Saving to: ‘NRC-Emotion-Lexicon.zip’\n",
            "\n",
            "NRC-Emotion-Lexicon 100%[===================>]  24.68M  6.63MB/s    in 3.7s    \n",
            "\n",
            "2023-01-10 15:20:05 (6.63 MB/s) - ‘NRC-Emotion-Lexicon.zip’ saved [25878449/25878449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get reddit sample\n",
        "!gdown https://drive.google.com/uc?id=1ogRFqUjIUqBxIUkGX6z8DobOWonOJp_u\n",
        "\n",
        "# get backend toolkit code\n",
        "!wget https://public-thought.media.mit.edu/static/ccc_toolkit_v_21_0.py --no-check-certificate\n",
        "\n",
        "# get emotions lexicon\n",
        "!wget https://saifmohammad.com/WebDocs/Lexicons/NRC-Emotion-Lexicon.zip\n",
        "!unzip -qq NRC-Emotion-Lexicon.zip\n",
        "\n",
        "# Download and load data (this is going to take a couple of minutes.)\n",
        "from ccc_toolkit_v_21_0 import (get_clusters,\n",
        "                          plot_tsne_viz,\n",
        "                          set_replicable_results,\n",
        "                          run_clustering,\n",
        "                          retrieve,\n",
        "                          SentenceTransformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dataset"
      ],
      "metadata": {
        "id": "Baxg7zEObPVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reddit Moderated Comments (Sample)\n",
        "\n",
        "Selected sample of comments that have been removed by Reddit moderators.\n"
      ],
      "metadata": {
        "id": "tgLTES_uQeUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "reddit_data = pd.read_csv('reddit_workshop_sample.csv')"
      ],
      "metadata": {
        "id": "N8_6rSnWQ1WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.columns"
      ],
      "metadata": {
        "id": "HDoT0jwvIUCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.shape"
      ],
      "metadata": {
        "id": "GS5H9MRgQ5QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data[:2]"
      ],
      "metadata": {
        "id": "xB2Roy3gRJzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.subreddit.value_counts()"
      ],
      "metadata": {
        "id": "-mXhV9gJTzjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data['violation reason'].value_counts()"
      ],
      "metadata": {
        "id": "6ENJ_Rw3IkIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data['moderator comment']"
      ],
      "metadata": {
        "id": "Ye-sBsRhT4rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxRb97zCboQo"
      },
      "source": [
        "# 2. Sentiment Lexicons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CYKnvorsLMP"
      },
      "source": [
        "We might be interested in when analyzing the would be how emotions expressed in these comments.\n",
        "\n",
        "Natural language processing has some techniques we can use to understand the emotional arc of conversations!\n",
        "\n",
        "This field of NLP is called \"sentiment analysis.\" \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQzqIG_khfId"
      },
      "source": [
        "We will use a list of words called a lexicon to make an emotion classifier, a model which will tell us whether a sentence is generally positive or negative emotion in nature.\n",
        "\n",
        "We can make this simple classifier using a sentiment and emotion lexicon called [Emolex](http://saifmohammad.com/WebPages/lexicons.html).\n",
        "\n",
        "Each word in the sentiment lexicon is tagged with either 'positive', 'negative', or even both in rare cases. Each word in the emotion lexicon is tagged with one of the 8 emotions according to Plutchik's wheel of emotions -- joy, trust, fear, surprise, sadness, anticipation, anger, and disgust. This is, of course, just one theory of emotion. Other lexicons are available within Emolex, labeling words from different domains along different dimensions (e.g., valence, arousal, dominance).\n",
        "\n",
        "These lexicons are typically created either (a) manually, through crowdsourced annotations, or (b) automatically (e.g., calculating co-ocurrence of words with each emotion word). The version we'll be using was created through crowdsourcing -- details can be found in the [paper](https://arxiv.org/pdf/1308.6297.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fEKfvstTOG"
      },
      "source": [
        "Let's first load in the lexicon and get a sense of what it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br04QmOubrab"
      },
      "outputs": [],
      "source": [
        "from ccc_toolkit_v_21_0 import parse_emolex\n",
        "\n",
        "lexicon_path = 'NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
        "word2sentiments, word2emotions = parse_emolex(lexicon_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gZG3_bwtYCi"
      },
      "source": [
        "Here are some labeled positive/negative sentiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9j0wqYej2dT"
      },
      "outputs": [],
      "source": [
        "print('Sentiment lexicon:\\n')\n",
        "for i, (word, sentiments) in enumerate(word2sentiments.items()):\n",
        "    print(word, sentiments)\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_DuZ6rttb-a"
      },
      "source": [
        "Here are some labeled emotions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsSe-vYlkK0y"
      },
      "outputs": [],
      "source": [
        "print('Emotion lexicon:\\n')\n",
        "for i, (word, emotions) in enumerate(word2emotions.items()):\n",
        "    print(word, emotions)\n",
        "    if i == 30:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyOc4sOMkP9Q"
      },
      "source": [
        "**Next, let's load load some comments and create a classifer with our lexicons.** Our approach is simple: a comment $x$ composed of a sequence of words $[w_0, w_1, ..., w_n]$, is classified as positive/negative or containing an emotion $e$ if any of the words $w_i$ is present in the lexicon. For example, a comment \"it was abnormal\" would be classified as containing the emotion \"digust\" because abnormal is mapped to disgust in the emotion lexicon.\n",
        "\n",
        "**Let's look at an example below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3hSDQofkMqg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def get_paragraph(string, words=10):\n",
        "    return '\\n'.join([' '.join(x) for x in chunks(string.split(), words)])\n",
        "\n",
        "comment = \"\"\"\n",
        "The way the current system is design has so many pitfalls, and it's all your fault.\n",
        "\"\"\"\n",
        "\n",
        "print(f'comment: {get_paragraph(comment)}\\n')\n",
        "print('Words in utterance tagged with emotions:\\n')\n",
        "word_list = re.sub(\"[^\\w]\", \" \",  comment).lower().split()\n",
        "\n",
        "for word in word_list:\n",
        "    if word in word2emotions:\n",
        "        print('\\t', word, word2emotions[word])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeQGGZ75oUgT"
      },
      "source": [
        "This approach has some shortcomings! One issue is the binary categorization of words as positive or negative.\n",
        "\n",
        "To overcome this, we will try a slightly different approach.\n",
        "\n",
        "**We'll use a more advanced lexicon-based (and rule-based) method called VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER consists of (1) a crowd-sourced valence-aware sentiment lexicon (i.e., each term has a *strength* associated with it), and 5 rules to incorporate features such as word-order sensitive relationships between terms.** These include intensifying valence through punctuation or capitlization, degree modifiers through adverbs, contrastive conjunction as a mixed signal with the latter clause dominating (e.g., \"The food here is great, but the service is horrible.\"), and negation of sentiment by preceding terms. The final sentiment is ultimately a sum of each word's valence and incorporation of these rules. See the [paper](http://eegilbert.org/papers/icwsm14.vader.hutto.pdf) and [repository of code](https://github.com/cjhutto/vaderSentiment) for more details.\n",
        "\n",
        "Though we are using it on conversation data, VADER also works well for social media posts, as it contains slang,\n",
        "commonly misspelled words, and emoticons in its lexicon (though some of these may be outdated now, as VADER came out in 2014)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFwIci4fleQK"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.columns"
      ],
      "metadata": {
        "id": "Q25JgjmuJRWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAtC07Q1odnR"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "comment_valences = []\n",
        "for i, comment in enumerate(reddit_data.text):\n",
        "    comment_valences.append(analyzer.polarity_scores(comment))\n",
        "reddit_data['vader_sentiment'] = comment_valences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb08IH00uFP-"
      },
      "source": [
        "Let's see what VADER tells us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ou9xObQpSIS"
      },
      "outputs": [],
      "source": [
        "for i, (_, row) in enumerate(reddit_data.iterrows()):\n",
        "    if i == 5: break\n",
        "    print(get_paragraph(row.text))\n",
        "    print('>>', row.vader_sentiment)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZMrYiLopoJd"
      },
      "source": [
        "The `compound` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A brief ending note on classifers\n",
        "\n",
        "We used lexicon- and rule-based classifiers in our labs today. Another approach are *feature-based* classifiers. In traditional machine learning models, a *feature extraction* step is first applied to an input text, extracting up to hundreds of lexical features about the words present, parts of speech present, ordering features, lexicon-based features, etc. Assuming a labeled dataset $X,Y$, where $X$ are the input texts and $Y$ are the ground-truth labels, the model learns probabilistically to predict a label $y$ given the extracted features of $x$.\n",
        "\n",
        "In deep neural networks (DNNs), this feature extraction step is skipped. Instead of having to hand-engineer features, DNNs can automatically learn features that are helpful for predicting $y$. Typically, these automatically-derived features will include the hand-engineered features -- see [1](https://www.aclweb.org/anthology/N19-1419.pdf), [2](https://hal.inria.fr/hal-02131630/document), etc.\n",
        "\n",
        "Deep neural networks and other machine learning methods now typically produce better results than lexicon-based approaches. However, a lexicon-based approach can still be useful, as shown with VADER, and be (a) a strong baseline, (b) useful for producing weak labels to train a machine learning model, and (c) act as a sanity check for a machine learning model (e.g., a neural-network based toxicity classifier such as [Perspective API](https://www.perspectiveapi.com/#/home) correlates extremely strongly with a simple expletive-lexicon-based classifier on Facebook comments.)"
      ],
      "metadata": {
        "id": "Q0BvVrHkm5iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Perspective API"
      ],
      "metadata": {
        "id": "IkmBuWYSeF7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Perspective is a free API that helps you host better conversations online.\n",
        "The API uses machine learning models to score the perceived impact a comment\n",
        "might have on a conversation. You can use this score to give feedback to\n",
        "commenters, help moderators more easily review comments, allow readers\n",
        "to more easily find interesting or productive comments, and more.\"\n",
        "\n",
        "https://developers.perspectiveapi.com/s/?language=en_US"
      ],
      "metadata": {
        "id": "9yNWo7TIenDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey='AIzaSyBMQ_87KcUcRFTHlEus8JYxzuy7wBAVaI0',\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "text = reddit_data.text[0]\n",
        "print(text, '\\n\\n------')\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': text },\n",
        "  'requestedAttributes': {'TOXICITY': {},\n",
        "                          'THREAT': {},\n",
        "                          'INSULT': {},\n",
        "                          'IDENTITY_ATTACK': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(json.dumps(response, indent=2))"
      ],
      "metadata": {
        "id": "11iZAhDicD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9zKKphYcD2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEe2JxVZphRa"
      },
      "source": [
        "# 4. Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-m-17Wfp9PU"
      },
      "source": [
        "#### Load models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMKzoRcHpUJ"
      },
      "source": [
        "Again, this may take a minute or two. Head on over to the next section while you're waiting for this to finish!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFd77gwkFF3"
      },
      "source": [
        "# https://nlp.stanford.edu/projects/glove/\n",
        "# https://github.com/RaRe-Technologies/gensim-data\n",
        "\n",
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\") # \"glove-twitter-50\"\n",
        "glove_model_vocab = glove_model.wv.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbyP0GIbJtfW"
      },
      "source": [
        "### You shall know a word by the company it keeps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dqnsoMuDAI"
      },
      "source": [
        "A starting question in natural language processing is how to represent words and text. As we saw previously, one simple approach is called \"bag of words\" (BoW), which simply counts the number of times each word appears. In this approach, words are represented as a \"one-hot vector\", corresponding to a vector of 0's and a 1 for the index of that word. For example, if our vocabulary consists of `['drink', 'eat' 'pray', 'love', 'earthquake']`, then `drink = [1,0,0,0,0]` and `love = [0,0,0,1,0]`.\n",
        "\n",
        "--- \n",
        "\n",
        "While simple, one drawback to this approach is that semantically related words don't have similar representations. For instance, we might want \"eat\" and \"drink\" to be more similar to each other than \"eat\" and \"earthquake\". In order to measure similarity, we must first have a distance metric. One common distance metric is Euclidean distance, defined between two vectors $p$ and $q$ as $d(p,q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i) ^ 2}$. However, note that $d(eat,drink) = d(eat,earthquake)$. In fact, all of the words are equally similar using this one-hot representation.\n",
        "\n",
        "--- \n",
        "\n",
        "The goal is to thus learn representations that can better capture notions of semantic and lexical similarity. The **Word2vec** model is one popular such approach. Each word is initialized with a randomly distributed (typically Gaussian-like) vector representation. Models are trained on large corpora to learn which words frequently co-occur together. For instance, given the context `the hungry hippo ____ his meal`, the model learns to predict that `devoured` and `ate` are reasonable words. Over the course of training, these vector representations are improved such that (1) they can better predict neighboring words, and, as a natural byproduct, (2) they better capture the semantic similarity we desired.\n",
        "\n",
        "---\n",
        "\n",
        "Let's take a look at what exactly we mean by this vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMBg_Y3Dxylp"
      },
      "source": [
        "print('Each word is represented as a {}-dimensional vector'.format(\n",
        "    glove_model.wv['orange'].shape[0]))\n",
        "print(glove_model.wv['orange'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvNQJdzUJ4-v"
      },
      "source": [
        "### Semantic neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDB2fsTra5xp"
      },
      "source": [
        "Let's look at what we can do once we have these vector representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88y8c4g1WLJ5"
      },
      "source": [
        "**First, a sanity check. Let's compute similarity scores between pairs of words.** The default distance metric is cosine similarity, which accounts for the *direction* of the vectors while normalizing for the *magnitude*. This score ranges from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBAiqw8COIFW"
      },
      "source": [
        "def compute_similarity(pairs, model):\n",
        "    \"\"\"\n",
        "    This is a helper function to print the similarity of each\n",
        "    pair of words in pairs. The output is printed in sorted order\n",
        "    of similarity.\n",
        "\n",
        "    Args:\n",
        "        pairs: list of tuples, each tuple contains two strings\n",
        "        model: Gensim word2vec model with a similarity() function\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for w1, w2 in pairs:\n",
        "        sim = model.similarity(w1, w2)\n",
        "        results.append((w1, w2, sim))\n",
        "\n",
        "    for w1, w2, sim in sorted(results, key=lambda x: -x[2]):\n",
        "        # print('%r\\t%r\\t%.2f' % (w1, w2, sim))\n",
        "        print('{:>12}\\t{:>12}\\t{:.2f}'.format(w1, w2, sim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4hF4o4n-KC"
      },
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "\n",
        "compute_similarity(pairs, glove_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7Tzm4fWEhu"
      },
      "source": [
        "**Perhaps you're interested in something a little trickier, such as the similarity of fruits, colors, or size adjectives.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnO4ZIWxOdF1"
      },
      "source": [
        "items = ['apple', 'cantaloupe', 'banana', 'coconut', 'pineapple', 'watermelon']\n",
        "# items = ['red', 'orange', 'yellow', 'blue', 'green', 'indigo', 'violet']\n",
        "# items = ['big', 'large', 'huge', 'enormous', 'gargantuan', 'vast']\n",
        "pairs = itertools.combinations(items, 2)  # creates pairwise combinations\n",
        "\n",
        "compute_similarity(pairs, glove_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ru-rTiPV10a"
      },
      "source": [
        "**We can also find a word's nearest neighbors.**\n",
        "\n",
        "For the word 'plum' (example below), these may include other fruits, specific plums (Kakadu plum), and also the notion of a \"plum\" job (sinecure, cushy, plum assignments, coveted). You may wonder why actor Yeager Lithgow apepars further down\n",
        "the list. A quick Google search surfaces a number of \n",
        "articles (on which this Word2vec model was trained) about\n",
        "\"[his wife] pushing him into the plum job\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSRknAguK7nC"
      },
      "source": [
        "result = glove_model.most_similar(positive=['plum'], topn=15)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7KKZnIMWWix"
      },
      "source": [
        "**Remember that word embeddings are trained by learning which words co-occur with other words. This means that the *.most_similar()* function doesn't necessariliy return words with the same *meaning* -- instead, it returns words that frequently occur with that word.**\n",
        "\n",
        "For example, while most of the most similar words for 'happy' are positive, 'disappointed' and even *'unhappy'* are within the top 15 matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0gHVPXHRycC"
      },
      "source": [
        "result = glove_model.most_similar(positive=['happy'], topn=20)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcefSRMrW89A"
      },
      "source": [
        "**We can also find words that are similar to *multiple* words.** This corresponds to finding words in vector space near the average of the two words (or centroid if multiple words are given)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kchrXpqW8CA"
      },
      "source": [
        "result = glove_model.most_similar(positive=['french', 'pastry'], topn=5)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSCeTdTMK945"
      },
      "source": [
        "**Word embeddings are also famous for being able to reconstruct analogies of the form `A is to B as C is to ___`.** For example, if we calculate `king + woman - man`, the closest word embedding is `queen`! (There are some [nuances](https://www.facebook.com/groups/1174547215919768/permalink/1846673885373761/)). \n",
        "\n",
        "Let's try it out for ourselves. In the following, we're finding the terms most similar to both woman and king, but dissimilar from man."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY1PSEBsoEks"
      },
      "source": [
        "print('King + woman - man:')\n",
        "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "\n",
        "\n",
        "# You'll see that warmer appears as a top term.\n",
        "# This is due to the co-ocurrence nature we mentioned before, where\n",
        "# colder and warmer are likely to occur in similar contexts, and hence\n",
        "# have similar vector representations. \n",
        "print('Louder + cold - loud:')\n",
        "result = glove_model.most_similar(positive=['louder', 'cold'], negative=['loud'], topn=3)\n",
        "pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvlUbLPQbAMi"
      },
      "source": [
        "**Finally, note that word embeddings may contain [\"human-like\" biases](https://science.sciencemag.org/content/356/6334/183.abstract)**. For example, if we compute `computer_programmer + woman - man`, the top result is `homemaker`. See for yourself below.\n",
        "\n",
        "These biases can have harmful repercussions on downstream tasks. There are [methods to debias](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf) these embeddings, but there are [limitations](https://arxiv.org/pdf/1903.03862.pdf) to these methods as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_4lf3-va_di"
      },
      "source": [
        "print('Doctor + woman - man:')\n",
        "result = glove_model.most_similar(positive=['doctor', 'woman'],\n",
        "                                   negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "\n",
        "print('Computer + woman - man:')\n",
        "result = glove_model.most_similar(positive=['computer', 'woman'],\n",
        "                                   negative=['man'], topn=3)\n",
        "pprint(result)\n",
        "\n",
        "print('\\n' + '-' * 100 + '\\n')\n",
        "print('Similarity between \"criminal\" and various names')\n",
        "pairs = [\n",
        "    ('criminal', 'matthew'),\n",
        "    ('criminal', 'bob'),\n",
        "    ('criminal', 'jake'),\n",
        "    ('criminal', 'darnell'),\n",
        "    ('criminal', 'trayvon'),\n",
        "    ('criminal', 'deshawn'),\n",
        "    ('criminal', 'alexander'),\n",
        "    ('criminal', 'aleksander'),\n",
        "    ('criminal', 'camilo'),\n",
        "    ('criminal', 'belén'),\n",
        "]\n",
        "compute_similarity(pairs, glove_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q89I0RYz4pLe"
      },
      "source": [
        "# 5. Embedding-based text retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGXBR7T8KDSJ"
      },
      "source": [
        "When we come upon a particularly interesting review, we might be interested in retrieving text from all conversations that matches that topic.\n",
        "\n",
        "One way we can do this is through a \"text retrieval\" approach. \n",
        "\n",
        "Our problem setup is: given a query $q$ (\"cancelled flight\") and a set of documents $D$ (our set of comments), we want to retrieve (and rank) the $n$ documents that most closely match our query. A query can be any natural language string (e.g. a word, a phrase, a sentence, a paragraph, etc.). Each document can similarly be any natural language string.\n",
        "\n",
        "We will be using a model called [Sentence-BERT](https://arxiv.org/abs/1908.10084), which computes a high dimensional vector representation of each sentence. It's an exciting method that would deserve its own workshop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx_r0BATaEVp"
      },
      "source": [
        "**Let's first load the model and the data.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBpgE12trH0C"
      },
      "outputs": [],
      "source": [
        "# There are different versions of the model, specified by the name 'bert-base...'\n",
        "\n",
        "sent_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kgsq26KwMiY"
      },
      "outputs": [],
      "source": [
        "# we are only using a subset of LVN public data.\n",
        "# with open('lvn_subset_data.pickle', 'rb') as handle:\n",
        "#    lvn_data = pickle.load(handle)\n",
        "import numpy as np\n",
        "np.random.seed(111)\n",
        "\n",
        "text = reddit_data.text.values.tolist()\n",
        "\n",
        "# alternatively, we can run BERT model to get embeddings, which takes ~30 mins.\n",
        "comments_embeddings = sent_model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "print(\"Sampled data contains {} comments.\".format(len(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dauy1zZdo-pm"
      },
      "source": [
        "What does an \"embedding\" representation look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIr8rORgrguP"
      },
      "outputs": [],
      "source": [
        "comments_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID9Xya3janX4"
      },
      "source": [
        "**Next, let's define the retrieval function and perform some searches.** For each query, we embed it using the same SentenceModel we used to embed the utterances. This means that both queries and utterances now live in the same high-dimensional vector space (to some degree...). We can now compute the cosine similarity between a query and each utterance to retrieve the most similar utterances in LVN conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN7Yq8WNSvFW"
      },
      "outputs": [],
      "source": [
        "queries = [\"it's your fault. You insulted her.\"]\n",
        "\n",
        "retrieve(sent_model, queries, text, comments_embeddings, closest_n=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCdoDwAbbWz2"
      },
      "source": [
        "**Great. Try creating some of your own queries.** Some questions you may be interested in asking include:\n",
        "- Do queries that are paraphrases of each other return similar matches?\n",
        "- For a query of your choosing, how does the average similarity score (over the top n results) compare between queries? What does this say about the relevance of that query / topic?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0c8OF9Sbbfw"
      },
      "outputs": [],
      "source": [
        "# Enter your own query between the quotation marks of your_query below!\n",
        "your_query = [\"\"\"My neighbour who sucks at farming but is good at\n",
        "making wooden wheels makes a deal with me. I will work on his farm,\n",
        "give him a share of the crops every harvest, and in return I get\n",
        "to sell the other portion. Is that slavery? How is that possibly slavery?\"\"\"]\n",
        "retrieve(sent_model, your_query, text, comments_embeddings, closest_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg3dV8CNp48g"
      },
      "source": [
        "# 6. Clustering and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjiqWHgpY4h"
      },
      "source": [
        "From the activity above, we see there is some overlap in the topics/sentiment that people bring up across comments.\n",
        "\n",
        "Assuming we don't want to think of and retrieve each topic individually, how might we sort and visualize our entire set of conversations?\n",
        "\n",
        "One approach would be through **unsupervised clustering.** A popular method for this is called K-means clustering. Let's watch a short video about K-means below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6szkarzv4KK"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!--\n",
        "\"\"\"\n",
        "Here the algorithm working on a dataset of 300 \"documents\" embedded to\n",
        "two dimensions.\n",
        "\"\"\"\n",
        "-->\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5I3Ei69I40s\"\n",
        "frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\"\n",
        "allowfullscreen>\n",
        "</iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSElL0Gfpy4k"
      },
      "source": [
        "Next, let's implement K-means clustering and visualize its results in 2-dimensional space.\n",
        "\n",
        "K-means clusters data points in high dimensions.\n",
        "- Originally we have **768** dimensions from **BERT**.\n",
        "- To run **k-mean** we need to compress that space to a lower dimensionality one. We do that by using **PCA** to capture as much linear variance (information) as possible.\n",
        "- We run **k-mean** on those lower-dimensionality vectors.\n",
        "- Finally, we reduce dimensionality to 2D using **t-SNE** for visualization purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvDOON2Kq3WZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Every time you run this cell you'll get different clusters and embeddings\n",
        "due to the fact that these methods calculate approximate transformations\n",
        "that depend on the initial random seed.\n",
        "\"\"\"\n",
        "set_replicable_results(True)\n",
        "\n",
        "viz_coord, clstr_model, predicted_clusters, _, _ = run_clustering(text,\n",
        "                                                                  pca_components=20,\n",
        "                                                                  k_clusters=20,\n",
        "                                                                  embeddings=comments_embeddings.cpu())\n",
        "\n",
        "# hover the dots in the plot with your mouse\n",
        "# to see where each piece of text got located.\n",
        "\n",
        "plot_tsne_viz(viz_coord, text, title=\"T-SNE<br>Hover the dots in the plot with your mouse.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_G3OzvKyHpO"
      },
      "source": [
        "It's hard to make sense of this. There aren't clear clusters, and it doesn't tell us anything important without additional manual labor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suEy5IzQrd_m"
      },
      "source": [
        "Let's color our 2D visualization according to the cluster ids we found through k-means. How does it look?\n",
        "\n",
        "Do we see any clustering that we would expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1lyM3UUqCHz"
      },
      "outputs": [],
      "source": [
        "# Predicted clusters\n",
        "plot_tsne_viz(viz_coord, text,\n",
        "                  clusters = predicted_clusters, \n",
        "                  coloring='clusters',\n",
        "                  title=\"K-means clusters. We ran k-means and got the following clusters:\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E6rjRQDH93cP",
        "hWXr0TbU_oGF",
        "GzVdhct0tzSN",
        "Baxg7zEObPVk",
        "tgLTES_uQeUl",
        "YxRb97zCboQo",
        "Q0BvVrHkm5iu",
        "IkmBuWYSeF7T",
        "xEe2JxVZphRa",
        "r-m-17Wfp9PU",
        "WbyP0GIbJtfW",
        "dvNQJdzUJ4-v",
        "Q89I0RYz4pLe",
        "tg3dV8CNp48g"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}